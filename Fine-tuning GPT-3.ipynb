{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3429d852",
   "metadata": {},
   "source": [
    "# Text summarization with T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14c4fc",
   "metadata": {},
   "source": [
    "## Step-1 Initializing the T5-large transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6153faa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/nitish/anaconda3/lib/python3.11/site-packages (4.41.0.dev0)\n",
      "Requirement already satisfied: filelock in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (2017.4.5)\n",
      "Requirement already satisfied: requests in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nitish/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nitish/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nitish/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nitish/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nitish/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nitish/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nitish/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418cfa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/nitish/anaconda3/lib/python3.11/site-packages (0.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62228daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_architecture=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa74454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b5342c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer,T5ForConditionalGeneration,T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522b1f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623137eb53284783a0741211d7bdd247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model=T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer=T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3943fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"t5-large\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 4096,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 16,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if display_architecture:\n",
    "    print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ef3ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if display_architecture:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6db1983d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Stack(\n",
      "  (embed_tokens): Embedding(32128, 1024)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 16)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseActDense(\n",
      "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1-23): 23 x T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseActDense(\n",
      "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if display_architecture:\n",
    "    print(model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab63920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Stack(\n",
      "  (embed_tokens): Embedding(32128, 1024)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 16)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseActDense(\n",
      "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1-23): 23 x T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerCrossAttention(\n",
      "          (EncDecAttention): T5Attention(\n",
      "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (2): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseActDense(\n",
      "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if display_architecture:\n",
    "    print(model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "459cbab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method T5ForConditionalGeneration.forward of T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 1024)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "if display_architecture:\n",
    "    print(model.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44a8482",
   "metadata": {},
   "source": [
    "## Creating a summarization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ae393ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text,ml):\n",
    "    preprocess_text=text.strip().replace('\\n','')\n",
    "    t5_prepared_text='summarize: '+preprocess_text\n",
    "    print('Preprocessed and prepared text : \\n',t5_prepared_text)\n",
    "    tokenized_text=tokenizer.encode(t5_prepared_text,return_tensors='pt')\n",
    "    summary_ids=model.generate(tokenized_text,num_beams=4,no_repeat_ngram_size=2,min_length=30,max_length=ml,early_stopping=True)\n",
    "    output=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7dae4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "The United States Declaration of Independence was the first Etext\n",
    "released by Project Gutenberg, early in 1971.\n",
    "The title was stored\n",
    "in an emailed instruction set which required a tape or diskpack be\n",
    "hand mounted for retrieval.\n",
    "The diskpack was the size of a large\n",
    "cake in a cake carrier, cost $1500, and contained 5 megabytes, of\n",
    "which this file took 1-2%.\n",
    "paper tape.\n",
    "Two tape backups were kept plus one on\n",
    "The 10,000 files we hope to have online by the end of\n",
    "2001 should take about 1-2% of a comparably priced drive in 2001.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c582590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 530\n"
     ]
    }
   ],
   "source": [
    "print('Number of characters:',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4628a2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and prepared text : \n",
      " summarize: The United States Declaration of Independence was the first Etextreleased by Project Gutenberg, early in 1971.The title was storedin an emailed instruction set which required a tape or diskpack behand mounted for retrieval.The diskpack was the size of a largecake in a cake carrier, cost $1500, and contained 5 megabytes, ofwhich this file took 1-2%.paper tape.Two tape backups were kept plus one onThe 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in 2001.\n"
     ]
    }
   ],
   "source": [
    "summary=summarize(text,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "990dc913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in summary: 231\n"
     ]
    }
   ],
   "source": [
    "print('Number of characters in summary:',len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1b8c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"\"\"\n",
    "No person shall be held to answer for a capital, or otherwise infamous\n",
    "crime,\n",
    "unless on a presentment or indictment of a Grand Jury, except in cases\n",
    "arising\n",
    "in the land or naval forces, or in the Militia, when in actual service\n",
    "in time of War or public danger; nor shall any person be subject for\n",
    "the same offense to be twice put in jeopardy of life or limb;\n",
    "nor shall be compelled in any criminal case to be a witness against\n",
    "himself,\n",
    "nor be deprived of life, liberty, or property, without due process of law;\n",
    "nor shall private property be taken for public use without just\n",
    "compensation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cc6ad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 590\n"
     ]
    }
   ],
   "source": [
    "print('Number of characters:',len(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19628df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and prepared text : \n",
      " summarize: No person shall be held to answer for a capital, or otherwise infamouscrime,unless on a presentment or indictment of a Grand Jury, except in casesarisingin the land or naval forces, or in the Militia, when in actual servicein time of War or public danger; nor shall any person be subject forthe same offense to be twice put in jeopardy of life or limb;nor shall be compelled in any criminal case to be a witness againsthimself,nor be deprived of life, liberty, or property, without due process of law;nor shall private property be taken for public use without justcompensation.\n"
     ]
    }
   ],
   "source": [
    "summary1=summarize(text1,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb55e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in summary: 255\n"
     ]
    }
   ],
   "source": [
    "print('Number of characters in summary:',len(summary1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "429ac834",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"The law regarding corporations prescribes that a corporation can\n",
    "be incorporated in the state of Montana to serve any lawful purpose. In\n",
    "the state of Montana, a corporation has all the powers of a natural person\n",
    "for carrying out its business activities. The corporation can sue and be\n",
    "sued in its corporate name. It has perpetual succession. The corporation\n",
    "can buy, sell or otherwise acquire an interest in a real or personal\n",
    "property. It can conduct business, carry on operations, and have offices\n",
    "and exercise the powers in a state, territory or district in possession of\n",
    "the U.S., or in a foreign country. It can appoint officers and agents of\n",
    "the corporation for various duties and fix their compensation.\n",
    "The name of a corporation must contain the word \"corporation\" or its\n",
    "abbreviation \"corp.\" The name of a corporation should not be deceptively\n",
    "similar to the name of another corporation incorporated in the same state.\n",
    "It should not be deceptively identical to the fictitious name adopted by a\n",
    "foreign corporation having business transactions in the state.\n",
    "The corporation is formed by one or more natural persons by executing and\n",
    "filing articles of incorporation to the secretary of state of filing. The\n",
    "qualifications for directors are fixed either by articles of incorporation\n",
    "or bylaws. The names and addresses of the initial directors and purpose\n",
    "of incorporation should be set forth in the articles of incorporation.\n",
    "The articles of incorporation should contain the corporate name, the\n",
    "number of shares authorized to issue, a brief statement of the character\n",
    "of business carried out by the corporation, the names and addresses of\n",
    "the directors until successors are elected, and name and addresses of\n",
    "incorporators. The shareholders have the power to change the size of\n",
    "board of directors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9567acb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 1804\n"
     ]
    }
   ],
   "source": [
    "print('Number of characters:',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bdbf766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and prepared text : \n",
      " summarize: The law regarding corporations prescribes that a corporation canbe incorporated in the state of Montana to serve any lawful purpose. Inthe state of Montana, a corporation has all the powers of a natural personfor carrying out its business activities. The corporation can sue and besued in its corporate name. It has perpetual succession. The corporationcan buy, sell or otherwise acquire an interest in a real or personalproperty. It can conduct business, carry on operations, and have officesand exercise the powers in a state, territory or district in possession ofthe U.S., or in a foreign country. It can appoint officers and agents ofthe corporation for various duties and fix their compensation.The name of a corporation must contain the word \"corporation\" or itsabbreviation \"corp.\" The name of a corporation should not be deceptivelysimilar to the name of another corporation incorporated in the same state.It should not be deceptively identical to the fictitious name adopted by aforeign corporation having business transactions in the state.The corporation is formed by one or more natural persons by executing andfiling articles of incorporation to the secretary of state of filing. Thequalifications for directors are fixed either by articles of incorporationor bylaws. The names and addresses of the initial directors and purposeof incorporation should be set forth in the articles of incorporation.The articles of incorporation should contain the corporate name, thenumber of shares authorized to issue, a brief statement of the characterof business carried out by the corporation, the names and addresses ofthe directors until successors are elected, and name and addresses ofincorporators. The shareholders have the power to change the size ofboard of directors.\n"
     ]
    }
   ],
   "source": [
    "summary=summarize(text,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec2d0e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a corporation in the state of Montana can serve any lawful purpose. the corporation can sue and besued in itscorporate name - and have perpetual succession if it buys, sells or otherwise acquires\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6aba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
